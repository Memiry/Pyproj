# -*- coding: utf-8 -*-


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix


sns.set(style="whitegrid")
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class IoTTrafficSimulator:

    def __init__(self, n_samples=2000):
        self.n_samples = n_samples
        # å®šä¹‰è®¾å¤‡é…ç½® (Paper Table I)
        # size_mu: åŒ…å¤§å°å‡å€¼, size_std: åŒ…å¤§å°æ ‡å‡†å·®
        # iat_mu: åˆ°è¾¾é—´éš”å‡å€¼, iat_std: åˆ°è¾¾é—´éš”æ ‡å‡†å·®
        # proto: åè®® (0=TCP, 1=UDP)
        self.devices = {
            'æ™ºèƒ½æ‘„åƒå¤´ (Camera)': {'size_mu': 1200, 'size_std': 200, 'iat_mu': 0.05, 'iat_std': 0.02, 'proto': 1},
            'æ™ºèƒ½æ’åº§ (Plug)':     {'size_mu': 64,   'size_std': 10,  'iat_mu': 5.0,  'iat_std': 2.0,  'proto': 0},
            'è¯­éŸ³åŠ©æ‰‹ (Voice)':    {'size_mu': 300,  'size_std': 150, 'iat_mu': 0.5,  'iat_std': 0.3,  'proto': 0},
            'æ¸©æ§å™¨ (Thermostat)': {'size_mu': 128,  'size_std': 20,  'iat_mu': 60.0, 'iat_std': 5.0,  'proto': 0}
        }

    def generate(self):
        print("ğŸ”„æ ¹æ®ç»Ÿè®¡æ¨¡å‹ç”Ÿæˆä»¿çœŸæµé‡æ•°æ®...")
        data = []
        labels = []
        
        samples_per_device = self.n_samples // len(self.devices)

        for device_name, stats in self.devices.items():
            for _ in range(samples_per_device):
                # å®ç°å…¬å¼ (1) å’Œ (2): S ~ N(mu, sigma), I ~ N(mu, sigma)
                pkt_size_mean = np.random.normal(stats['size_mu'], stats['size_std'])
                pkt_size_std  = abs(np.random.normal(stats['size_std'], 5))
                iat_mean      = abs(np.random.normal(stats['iat_mu'], stats['iat_std']))
                iat_std       = abs(np.random.normal(stats['iat_std'], 0.01))
                proto         = stats['proto']
                
                # å™ªå£°æ³¨å…¥
                # æ¨¡æ‹Ÿ 5% çš„æ¦‚ç‡å‡ºç°ç½‘ç»œæŠ–åŠ¨å¯¼è‡´çš„å¼‚å¸¸åŒ…
                if np.random.random() < 0.05:
                    pkt_size_mean += np.random.randint(100, 500)
                
                # ç¡®ä¿æ•°æ®éè´Ÿ
                pkt_size_mean = max(0, pkt_size_mean)
                iat_mean = max(0.001, iat_mean)

                data.append([pkt_size_mean, pkt_size_std, iat_mean, iat_std, proto])
                labels.append(device_name)

        columns = ['åŒ…å¤§å°å‡å€¼', 'åŒ…å¤§å°æ–¹å·®', 'é—´éš”å‡å€¼', 'é—´éš”æ–¹å·®', 'åè®®ç±»å‹']
        df = pd.DataFrame(data, columns=columns)
        df['Label'] = labels
        return df

class ModelTrainer:
    """
   
    """
    def __init__(self, df):
        self.df = df
        self.X = df.drop('Label', axis=1)
        self.y = df['Label']
        self.scaler = StandardScaler()
        self.best_model = None
        self.X_test_scaled = None
        self.y_test = None
        self.classes = None

    def preprocess_and_split(self):
        # æ•°æ®åˆ’åˆ† (Paper Section V-A)
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42)
        
        # æ ‡å‡†åŒ– (Paper Section IV-A: Z-score normalization)
        X_train_scaled = self.scaler.fit_transform(X_train)
        self.X_test_scaled = self.scaler.transform(X_test)
        self.y_test = y_test
        
        return X_train_scaled, y_train

    def train_with_gridsearch(self, X_train, y_train):
        print("\nâš™ï¸ç½‘æ ¼æœç´¢ (Grid Search) è®­ç»ƒæ¨¡å‹...")
        
        # å®šä¹‰ Random Forest å’Œ SVM çš„å‚æ•°ç½‘æ ¼
        model_params = {
            'RandomForest': {
                'model': RandomForestClassifier(random_state=42),
                'params': {'n_estimators': [50, 100], 'max_depth': [10, 20]}
            },
            'SVM': {
                'model': SVC(probability=True, random_state=42),
                'params': {'C': [1, 10], 'kernel': ['rbf']}
            }
        }

        best_score = 0
        
        for name, config in model_params.items():
            print(f"   -> æ­£åœ¨ä¼˜åŒ– {name} ...")
            grid = GridSearchCV(config['model'], config['params'], cv=3, n_jobs=-1)
            grid.fit(X_train, y_train)
            
            print(f"      {name} æœ€ä½³å‡†ç¡®ç‡: {grid.best_score_:.4f} | å‚æ•°: {grid.best_params_}")
            
            if grid.best_score_ > best_score:
                best_score = grid.best_score_
                self.best_model = grid.best_estimator_
        
        self.classes = self.best_model.classes_
        print(f"âœ… æœ€ç»ˆé€‰æ‹©æ¨¡å‹: {type(self.best_model).__name__}")

class OpenSetDetector:

    def __init__(self, model, scaler, threshold=0.6):
        self.model = model
        self.scaler = scaler
        self.threshold = threshold

    def detect(self, features):
        print("\nğŸ›¡ï¸ å¼€æ”¾é›†å¼‚å¸¸æ£€æµ‹ (Open-Set Detection)")
        # æ ‡å‡†åŒ–è¾“å…¥ç‰¹å¾
        features_scaled = self.scaler.transform(features)
        
        # è·å–é¢„æµ‹æ¦‚ç‡ (Posterior Probability)
        probs = self.model.predict_proba(features_scaled)
        pred_indices = np.argmax(probs, axis=1)
        max_probs = np.max(probs, axis=1)
        
        results = []
        for i, prob in enumerate(max_probs):
            pred_class = self.model.classes_[pred_indices[i]]
            
    
            if prob < self.threshold:
                status = "ğŸš¨ è­¦æŠ¥: æœªçŸ¥è®¾å¤‡/æ½œåœ¨æ”»å‡» (Unknown)"
            else:
                status = f"âœ… è®¤è¯é€šè¿‡: {pred_class}"
            
            print(f"   è¾“å…¥å‘é‡: {features[i]}")
            print(f"   é¢„æµ‹ç±»: {pred_class} | ç½®ä¿¡åº¦: {prob:.4f} -> {status}")
            results.append(status)
        return results

class Visualizer:

    def __init__(self, model, X_test, y_test):
        self.model = model
        self.X_test = X_test
        self.y_test = y_test
        self.y_pred = model.predict(X_test)

    def plot_results(self):
        print("\nğŸ“Š [æ¨¡å— 4] ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        plt.figure(figsize=(14, 6))

        # 1. æ··æ·†çŸ©é˜µ (Fig. 1 in Paper)
        plt.subplot(1, 2, 1)
        cm = confusion_matrix(self.y_test, self.y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=self.model.classes_, yticklabels=self.model.classes_)
        plt.title('Fig. 1. Confusion Matrix (Test Data)')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')

        # 2. PCA é™ç»´å¯è§†åŒ–
        plt.subplot(1, 2, 2)
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(self.X_test)
        # å°† y_test è½¬æ¢ä¸ºé¢œè‰²ç´¢å¼•ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        unique_labels = list(set(self.y_test))
        # ä½¿ç”¨ seaborn ç»˜åˆ¶æ•£ç‚¹å›¾
        sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=self.y_test, palette='deep', s=60)
        plt.title('IoT Device Clusters (PCA Projection)')
        plt.xlabel('Principal Component 1')
        plt.ylabel('Principal Component 2')

        plt.tight_layout()
        plt.show()
        print("âœ… å›¾è¡¨å·²ç”Ÿæˆã€‚")

# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # 1. ç”Ÿæˆæ•°æ®
    simulator = IoTTrafficSimulator(n_samples=2000)
    df = simulator.generate()

    # 2. è®­ç»ƒæ¨¡å‹
    trainer = ModelTrainer(df)
    X_train, y_train = trainer.preprocess_and_split()
    trainer.train_with_gridsearch(X_train, y_train)

    # 3. å¯è§†åŒ–è¯„ä¼°
    viz = Visualizer(trainer.best_model, trainer.X_test_scaled, trainer.y_test)
    viz.plot_results()

    # æ„é€ ä¸€ä¸ªé«˜é¢‘ UDP æ”»å‡»æµ (åŒ…å¾ˆå¤§ï¼Œé—´éš”æçŸ­)
    # [åŒ…å¤§å°å‡å€¼, åŒ…å¤§å°æ–¹å·®, é—´éš”å‡å€¼, é—´éš”æ–¹å·®, åè®®]
    attack_vector = np.array([
        [2000, 500, 0.001, 0.001, 1],  # æ¨¡æ‹Ÿ DDoS æ”»å‡»
        [64, 5, 5.0, 0.1, 0]           # æ¨¡æ‹Ÿæ­£å¸¸çš„æ™ºèƒ½æ’åº§ (ç”¨äºå¯¹æ¯”)
    ])
    
    detector = OpenSetDetector(trainer.best_model, trainer.scaler, threshold=0.6)
    detector.detect(attack_vector)
    
